{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the lbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ['It was a nice ran day.', 'The things are so beautyful in his point.', \n",
    "'When your focus is clear, you won.', 'Many many happy returns of the day.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a nice ran day.',\n",
       " 'The things are so beautyful in his point.',\n",
       " 'When your focus is clear, you won.',\n",
       " 'Many many happy returns of the day.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'day': 2,\n",
       "             'a': 1,\n",
       "             'was': 1,\n",
       "             'it': 1,\n",
       "             'nice': 1,\n",
       "             'ran': 1,\n",
       "             'his': 1,\n",
       "             'so': 1,\n",
       "             'in': 1,\n",
       "             'things': 1,\n",
       "             'beautyful': 1,\n",
       "             'point': 1,\n",
       "             'are': 1,\n",
       "             'the': 2,\n",
       "             'is': 1,\n",
       "             'when': 1,\n",
       "             'focus': 1,\n",
       "             'won': 1,\n",
       "             'you': 1,\n",
       "             'your': 1,\n",
       "             'clear': 1,\n",
       "             'many': 1,\n",
       "             'of': 1,\n",
       "             'happy': 1,\n",
       "             'returns': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 1,\n",
       " 'the': 2,\n",
       " 'many': 3,\n",
       " 'it': 4,\n",
       " 'was': 5,\n",
       " 'a': 6,\n",
       " 'nice': 7,\n",
       " 'ran': 8,\n",
       " 'things': 9,\n",
       " 'are': 10,\n",
       " 'so': 11,\n",
       " 'beautyful': 12,\n",
       " 'in': 13,\n",
       " 'his': 14,\n",
       " 'point': 15,\n",
       " 'when': 16,\n",
       " 'your': 17,\n",
       " 'focus': 18,\n",
       " 'is': 19,\n",
       " 'clear': 20,\n",
       " 'you': 21,\n",
       " 'won': 22,\n",
       " 'happy': 23,\n",
       " 'returns': 24,\n",
       " 'of': 25}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = tokenizer.texts_to_matrix(lines)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 26)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 6, 7, 8, 1],\n",
       " [2, 9, 10, 11, 12, 13, 14, 15],\n",
       " [16, 17, 18, 19, 20, 21, 22],\n",
       " [3, 3, 23, 24, 25, 2, 1]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded =  pad_sequences(seq, maxlen=10, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6,  7,  8,  1,  0,  0,  0,  0],\n",
       "       [ 2,  9, 10, 11, 12, 13, 14, 15,  0,  0],\n",
       "       [16, 17, 18, 19, 20, 21, 22,  0,  0,  0],\n",
       "       [ 3,  3, 23, 24, 25,  2,  1,  0,  0,  0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  4,  5,  6,  7,  8,  1],\n",
       "       [ 0,  0,  2,  9, 10, 11, 12, 13, 14, 15],\n",
       "       [ 0,  0,  0, 16, 17, 18, 19, 20, 21, 22],\n",
       "       [ 0,  0,  0,  3,  3, 23, 24, 25,  2,  1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded =  pad_sequences(seq, maxlen=10, padding='pre')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
